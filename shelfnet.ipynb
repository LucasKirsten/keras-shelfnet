{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras ShelfNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic notebook options\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "import os\n",
    "# ------ choose GPUs to be used -------\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #gpu to be used\n",
    "\n",
    "from model import net, data\n",
    "from utils import *\n",
    "\n",
    "import subprocess\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "model_name = 'shelfnet'\n",
    "\n",
    "# dictionary to config the network parameters\n",
    "config = dict(\n",
    "    model_name    = model_name,\n",
    "    classes       = 5,\n",
    "    input_shape   = (224,224,3),\n",
    "    dropout       = 0.2,\n",
    "    learning_rate = 1e-3,\n",
    "    loss          = 'binary_crossentropy',\n",
    "    metrics       = [jaccard_distance, 'acc']\n",
    ")\n",
    "\n",
    "# # metric to be monitored during training (e.g. val_jaccard_distance_loss)\n",
    "monitor = 'val_loss' \n",
    "\n",
    "# paths to train and validation sets\n",
    "folders_train = ['path to dataset 1', 'path to dataset 2'] \n",
    "folders_test  = ['path to testset 1', 'path to testset 2'] # used during validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mixed precision training (not neccessary)\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)\n",
    "\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data loader\n",
    "loader = data.DataLoader(folders_train, folders_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize generator\n",
    "loader.view_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize generator\n",
    "loader.view_data('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Here all the train steps are made, such as loading the model and define the train options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "with tf.distribute.MirroredStrategy().scope():\n",
    "    model = net.Model(**config).build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "model.fit(\n",
    "    loader.flow('train', batch_size=32),\n",
    "    steps_per_epoch=loader.data_size('train')//32,\n",
    "\n",
    "    epochs=50,\n",
    "    callbacks=[callbacks.TerminateOnNaN(),\n",
    "               callbacks.ModelCheckpoint(f'results/{model.name}.h5',\n",
    "                                         monitor=monitor, mode='min',\n",
    "                                         verbose=1, save_best_only=True),\n",
    "               callbacks.EarlyStopping(monitor=monitor, patience=5, verbose=1,\n",
    "                                       mode='min', restore_best_weights=True),\n",
    "               callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.5, patience=2,\n",
    "                                           verbose=1, mode='min', min_lr=1e-6),\n",
    "               callbacks.CSVLogger(f'results/{model.name}_history.csv')],\n",
    "\n",
    "    validation_data=loader.flow('test', batch_size=2),\n",
    "    validation_steps=loader.data_size('test')//2,\n",
    "\n",
    "    use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Here we evaluate the model and write the results on a csv file that will contain all of the experiments results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weight's model from training\n",
    "model.load_weights(f'results/{model.name}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results on csv file\n",
    "losses = model.evaluate_generator(loader.flow('test', batch_size=1), steps=loader.data_size('test'), verbose=1)\n",
    "\n",
    "path_eval = f'results/{model.name}_validation.csv'\n",
    "with open(path_eval, 'a') as file:\n",
    "    if not os.path.exists(path_eval): # header\n",
    "        file.write(f\"Model name,{','.join(model.metrics_names)}\\n\")\n",
    "        \n",
    "    file.write(f\"{model.name},{','.join([str(l) for l in losses])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Here are codes to test the model visualizing the results of input and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the data from train or test set\n",
    "loader.predict_data(model, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on a non-labeled testset\n",
    "path_images = glob('path to a testset/*.jpg')\n",
    "\n",
    "plt.figure(figsize=(40,40))\n",
    "for i,p in enumerate(path_images):\n",
    "    x = imread(p, resize=(224,224))\n",
    "    \n",
    "    p = loader.predict_input(model, x)\n",
    "\n",
    "    plt.subplot(len(path_images)//4+1,4,i+1)\n",
    "    plt.imshow(np.hstack([x,p]))\n",
    "    plt.axis('off')\n",
    "plt.savefig(f'results/{model.name}_testset.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
